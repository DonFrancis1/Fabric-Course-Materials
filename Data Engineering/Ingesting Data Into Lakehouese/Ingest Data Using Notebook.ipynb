{
    "metadata": {
        "kernelspec": {
            "name": "SQL",
            "display_name": "SQL",
            "language": "sql"
        },
        "language_info": {
            "name": "sql",
            "version": ""
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": [
                " from pyspark.sql.types import *\n",
                "    \n",
                " # Create the schema for the table\n",
                " orderSchema = StructType([\n",
                "     StructField(\"SalesOrderNumber\", StringType()),\n",
                "     StructField(\"SalesOrderLineNumber\", IntegerType()),\n",
                "     StructField(\"OrderDate\", DateType()),\n",
                "     StructField(\"CustomerName\", StringType()),\n",
                "     StructField(\"Email\", StringType()),\n",
                "     StructField(\"Item\", StringType()),\n",
                "     StructField(\"Quantity\", IntegerType()),\n",
                "     StructField(\"UnitPrice\", FloatType()),\n",
                "     StructField(\"Tax\", FloatType())\n",
                "     ])\n",
                "    \n",
                " # Import all files from bronze folder of lakehouse\n",
                " df = spark.read.format(\"csv\").option(\"header\", \"true\").schema(orderSchema).load(\"Files/*.csv\")\n",
                "    \n",
                " # Display the first 10 rows of the dataframe to preview your data\n",
                " display(df.head(10))"
            ],
            "metadata": {
                "azdata_cell_guid": "2e1233e5-3926-4727-b0f3-160f109facc0",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Use format and save to load as a Delta table\r\n",
                "table_name = \"silver\"\r\n",
                "df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\r\n",
                "\r\n",
                "# Confirm load as Delta table\r\n",
                "print(f\"Spark DataFrame saved to Delta table: {table_name}\")"
            ],
            "metadata": {
                "azdata_cell_guid": "3004507a-d93d-47a5-b2dc-be85b5bbf6a8",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}